{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install wikipedia"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kfb16C1b5dE7",
        "outputId": "65886a0e-d62b-4b61-ad97-cb6acc33aa7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wikipedia\n",
            "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.9/dist-packages (from wikipedia) (4.11.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from wikipedia) (2.27.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.26.15)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.9/dist-packages (from beautifulsoup4->wikipedia) (2.4.1)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11696 sha256=c9df260fd6b76eb8b9803f1aaa04ad8fb0b69acde7f45dcedb2318b7d700ca14\n",
            "  Stored in directory: /root/.cache/pip/wheels/c2/46/f4/caa1bee71096d7b0cdca2f2a2af45cacf35c5760bee8f00948\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia\n",
            "Successfully installed wikipedia-1.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VlmgtnXB5Bmh"
      },
      "outputs": [],
      "source": [
        "import wikipedia as wiki\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "domains = ['weather', 'sports']"
      ],
      "metadata": {
        "id": "s0BvQcHZ7N44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weather_documents = ['Weather', 'Weather_forecasting', 'Weather modification', 'Weather Underground (weather service)', 'National Weather Service', 'AccuWeather']\n",
        "sports_documents = ['Sport', 'BeINSports', 'Sports Illustrated', 'Professional sports', 'Bally Sports', 'Olympic sports']"
      ],
      "metadata": {
        "id": "h1FUKQ1YHaWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def class_prob(doc_label):\n",
        "  # assuming classes numbers from 0 till n\n",
        "  number_of_classes = max(doc_label)+1\n",
        "  classes = [0] * number_of_classes\n",
        "  for label in doc_label:\n",
        "    classes[label] += 1\n",
        "  total_docs = len(doc_label)\n",
        "  classes_probability = [c/total_docs for c in classes]\n",
        "  return classes_probability"
      ],
      "metadata": {
        "id": "zNCSPgmpEtjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing(doc):\n",
        "\n",
        "  punctuations_numbers = '!@#$%^&*()~`,.;:/\"\\'[]{}\\\\<>?=+-_|1234567890'\n",
        "\n",
        "  stop_words = ['a', 'an', 'the', 'then', 'else', 'where', 'when', 'how', 'many', 'much',\n",
        "                'i', 'he', 'she', 'they', 'you', 'it', 'them', 'him', 'his', 'here', 'mine', 'our', \n",
        "                'ours', 'is', 'are', 'am', 'all', 'some', 'do', 'did', 'has', 'have', 'had', 'been',\n",
        "                'myself', 'yours', 'can', 'could', 'nor', 'no', 'not', 'too', 'so', 'very',\n",
        "                'nt', 'm', 've', 're', 'll', 's', 'd', 'yet', 'n', 't', 'at', 'from', 'those', 'be',\n",
        "                'other', 'others', 'such', 'most', 'but', 'now', 'then', 'later', 'soon', 'in',\n",
        "                'on', 'between', 'above', 'further', 'sooner', 'and', 'of', 'to', 'by', 'as', 'if',\n",
        "                'that', 'this', 'these', 'from', 'maybe']\n",
        "\n",
        "  preproceed_doc = \"\"\n",
        "  # remove all punctuations and numbers by replace them with space\n",
        "  for c in doc:\n",
        "    if c in punctuations_numbers:\n",
        "      preproceed_doc += ' '\n",
        "    else:\n",
        "      preproceed_doc += c\n",
        "\n",
        "  # remove all stop words and join the rest with single space\n",
        "  preproceed_doc = \" \".join([x for x in preproceed_doc.split() if x.lower() not in stop_words])\n",
        "  \n",
        "  return preproceed_doc"
      ],
      "metadata": {
        "id": "GvJ1KwCFF9ss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getTokens(doc):\n",
        "  words = [x.lower() for x in doc.split()]\n",
        "  tokens = list(set(words))\n",
        "  return tokens"
      ],
      "metadata": {
        "id": "RGyVJaAEPv40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def conditional_prob(docs_tokens, docs_labels):\n",
        "  word_conditional_probabilities = {}\n",
        "\n",
        "  unique_labels = set(docs_labels)\n",
        "  for label in unique_labels:\n",
        "    word_conditional_probabilities[label] = dict()\n",
        "  \n",
        "  all_tokens = set()\n",
        "\n",
        "  # count the number of times each token appear in the class\n",
        "  for tokens, label in zip(docs_tokens, docs_labels):\n",
        "    all_tokens.update(tokens)\n",
        "    current_class = word_conditional_probabilities[label]\n",
        "    for token in tokens:\n",
        "      if token not in current_class:\n",
        "        current_class[token] = 1\n",
        "      else:\n",
        "        current_class[token] += 1\n",
        "  \n",
        "  # find the probability of token given the class\n",
        "  for token in all_tokens:\n",
        "    # total number of times the token appear in the all classes \n",
        "    count = 0\n",
        "    for label in unique_labels:\n",
        "      if token in word_conditional_probabilities[label]:\n",
        "        count += word_conditional_probabilities[label][token]\n",
        "    # divide each number of times the token appear in the class by count\n",
        "    for label in unique_labels:\n",
        "       if token in word_conditional_probabilities[label]:\n",
        "         word_conditional_probabilities[label][token] /= count\n",
        "  return word_conditional_probabilities"
      ],
      "metadata": {
        "id": "bT-xexRRRITW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = []\n",
        "train_labels = [0]*5 + [1]*5\n",
        "test_data = []\n",
        "test_labels = [0, 1]\n",
        "\n",
        "for doc_name in weather_documents[:5]+sports_documents[:5]:\n",
        "  train_data.append(wiki.page(doc_name).content[:2000])\n",
        "\n",
        "for doc_name in weather_documents[5:]+sports_documents[5:]:\n",
        "  test_data.append(wiki.page(doc_name).content[:2000])"
      ],
      "metadata": {
        "id": "O-gXhxVABfUC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('train data')\n",
        "print('number of docs', len(train_data))\n",
        "print('docs labels', train_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LjRx9vJiEWtE",
        "outputId": "9211e94b-c3d7-4609-bdab-272d85ea6464"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train data\n",
            "number of docs 10\n",
            "docs labels [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('test data')\n",
        "print('number of docs', len(test_data))\n",
        "print('docs labels', test_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHgjncPREbR-",
        "outputId": "e56255fc-8038-4766-f739-a13ef37dec88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test data\n",
            "number of docs 2\n",
            "docs labels [0, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classes_probability = class_prob(train_labels)\n",
        "\n",
        "docs = [preprocessing(doc) for doc in train_data]\n",
        "docs_tokens = [getTokens(doc) for doc in docs]\n",
        "words_conditional_probability = conditional_prob(docs_tokens, train_labels)\n",
        "\n",
        "#save probabilities\n",
        "with open('model.pkl', 'wb') as f:\n",
        "    pickle.dump(classes_probability, f)\n",
        "    pickle.dump(words_conditional_probability, f)"
      ],
      "metadata": {
        "id": "pKWq9FOG4zkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to deal with zero probability, when the token has zero probability in the given class\n",
        "\"\"\"\n",
        "n_prob : observation probability\n",
        "N : total observations\n",
        "K : number of features\n",
        "alpha : smoothing parameter\n",
        "\"\"\"\n",
        "def laplace_smoothing_probability(n_prob, N, K, alpha=1):\n",
        "  return (n_prob*N + alpha) / (N + K * alpha)"
      ],
      "metadata": {
        "id": "_kStRr8zjpB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(test_doc):\n",
        "  # load probabilities\n",
        "  with open('model.pkl', 'rb') as f:\n",
        "    classes_probability = pickle.load(f)\n",
        "    words_conditional_prob = pickle.load(f)\n",
        "  \n",
        "  # prepare test document\n",
        "  test_doc = preprocessing(test_doc)\n",
        "  test_doc_tokens = getTokens(test_doc)\n",
        "  number_of_tokens = len(test_doc_tokens)\n",
        "\n",
        "  target_classes_labels = words_conditional_prob.keys()\n",
        "  number_of_classes = len(target_classes_labels)\n",
        "\n",
        "  probability_belong_to_target_class = [0]*number_of_classes\n",
        "\n",
        "  # calculate the probability of class given words using naive bayes approach\n",
        "  \"\"\"\n",
        "  p(c|w1,w2,..wn) = (p(w1|c)*p(w2|c)*..*p(wn|c)*p(c)) / [p(w1|c)*p(w2|c)*..*p(wn|c)*p(c) + p(w1|~c)*p(w2|~c)*..*p(wn|~c)*p(~c)]\n",
        "  let c1 = p(w1|c)*p(w2|c)*..*p(wn|c)*p(c), c2 = p(w1|~c)*p(w2|~c)*..*p(wn|~c)*p(~c)\n",
        "  p(c|w1,w2,..wn) = c1 / (c1+c2)\n",
        "  \"\"\"\n",
        "  for label in target_classes_labels:\n",
        "    c1 = classes_probability[label]\n",
        "    c2 = 1-classes_probability[label]\n",
        "    current_class_cond_prob = words_conditional_prob[label]\n",
        "\n",
        "    for token in test_doc_tokens:\n",
        "      # if token not appear in the give class, then it has zero probability in the given class\n",
        "      token_prob_given_class = 0\n",
        "      if token in current_class_cond_prob:\n",
        "        token_prob_given_class = current_class_cond_prob[token]\n",
        "      # as N unknown we will assume it as constant\n",
        "      c1 *= laplace_smoothing_probability(token_prob_given_class, N=1, K=number_of_tokens, alpha=1)\n",
        "      c2 *= laplace_smoothing_probability(1-token_prob_given_class, N=1, K=number_of_tokens, alpha=1)\n",
        "    \n",
        "    probability_belong_to_target_class[label] = c1/(c1+c2)\n",
        "  \n",
        "  max_probability = max(probability_belong_to_target_class)\n",
        "  winner_class = probability_belong_to_target_class.index(max_probability)\n",
        "  return probability_belong_to_target_class, winner_class"
      ],
      "metadata": {
        "id": "M3bxjcKd8JmH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for x in test_data:\n",
        "  probs, winner_class = predict(x)\n",
        "  print('probabilities:', probs)\n",
        "  print('belong to class:', winner_class)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PGpRwhbMqTpB",
        "outputId": "ebdddc8d-2c8b-4494-9af0-3d1685a6e0e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "probabilities: [2.969778618025228e-12, 9.437056727497458e-34]\n",
            "belong to class: 0\n",
            "probabilities: [2.7996882271025565e-22, 2.6871434370563706e-15]\n",
            "belong to class: 1\n"
          ]
        }
      ]
    }
  ]
}
